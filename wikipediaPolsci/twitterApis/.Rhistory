library(stringr) # string processing
library(rvest) # scraping suite
library(ggmap) # geocoding
## directory ---------------------
wd <- ("./ajpsReviewers")
dir.create(wd)
setwd(wd)
## step 1: inspect page
url <- "http://ajps.org/list-of-reviewers/"
browseURL(url)
content <- html(url)
content <- read_html(url)
anchors <- html_nodes(content, xpath = "//a")
hrefs <- html_attr(anchors, "href")
pdfs <- hrefs[ str_detect(hrefs, "reviewers.*\\d{4}.*pdf") ]
pdfs
pdfs <- hrefs[ str_detect(hrefs, "reviewer.*\\d{4}.*pdf") ]
pdfs <- pdfs[!is.na(pdfs)]
pdfs
pdfs <- hrefs[ str_detect(hrefs, "reviewer.*pdf") ]
pdfs <- pdfs[!is.na(pdfs)]
pdfs
pdf_names <- str_extract(pdfs, "\\d{4}.pdf")
pdf_names
pdf_names[1] <- "2015.pdf"
# download pdfs
for(i in seq_along(pdfs)) {
download.file(pdfs[i], pdf_names[i], mode="wb")
}
library(pdftools)
pdf_names
pdf_text(pdf_names[1])
pdftext <- pdf_text(pdf_names[1])
write(pdftext, file = "foo.txt")
seq_along(pdf_names)
pdf_names
for (i in seq_along(pdf_names)) {
pdftext <- pdf_text(pdf_names[i])
write(pdftext, file = paste0(str_extract(pdf_names[i], "[[:digit:]]{4}", ".txt"))
}
for (i in seq_along(pdf_names)) {
pdftext <- pdf_text(pdf_names[i])
write(pdftext, file = paste0(str_extract(pdf_names[i], "[[:digit:]]{4}", ".txt")))
}
for (i in seq_along(pdf_names)) {
pdftext <- pdf_text(pdf_names[i])
write(pdftext, file = paste0(str_extract(pdf_names[i], "[[:digit:]]{4}"), ".txt"))
}
## step 4: import data
txt_names <- list.files(pattern = "txt")
txt_names
## step 4: import data
txt_names <- list.files(pattern = "txt")
txt_names
rawdat <- readLines(txt_names[4])
txt_names
txt_names[4]
head(rawdat)
## step 5: tidy data
rev13 <- rawdat %>%
str_c(collapse="") %>%
str_replace_all(pattern = "[!\f]", replacement = " ")  %>%
str_replace_all(pattern = "\\]", replacement = " ") %>%
str_split(pattern = "\\)") %>%
unlist()
head(rev13)
rev13 <- rev13[-1]
names <- rev13 %>%
str_extract(pattern = "^.*?,") %>%
str_replace_all(pattern = " |,", replacement = " ") %>%
str_trim()
head(names)
institution <- rev13 %>%
str_extract(pattern = ",.*\\(") %>%
str_replace_all(pattern = " |\\(|^, ", replacement = " ") %>%
str_trim
head(institution)
reviews <- rev13 %>%
str_extract("\\(.*") %>%
str_extract("\\d+") %>%
as.numeric()
head(reviews)
sort(table(institution))
institution %>% table %>% sort
reviews <- rev13 %>%
str_extract("\\(.*") %>%
str_extract("\\d+") %>%
as.numeric()
head(reviews)
rev13_dat <- data.frame(names = names, institution = institution, reviews = reviews)
head(rev13_dat)
View(rev13_dat)
## step 6: geocode reviewers/institutions
# geocoding takes a while -> save results
# 2500 requests allowed per day
if ( !file.exists("institutions2013_geo.RData")){
pos <- geocode(rev13_dat$institution)
geocodeQueryCheck()
save(pos, file="institutions2013_geo.RData")
} else {
load("institutions2013_geo.RData")
}
head(pos)
rev13_dat$lon <- pos$lon
rev13_dat$lat <- pos$lat
## step 7: plot reviewers, worldwide
mapWorld <- borders("world")
map <-
ggplot() +
mapWorld +
geom_point(aes(x=rev13_dat$lon, y=rev13_dat$lat) ,
color="#F54B1A90", size=3 ,
na.rm=T) +
theme_bw() +
coord_map(xlim=c(-180, 180), ylim=c(-60,70))
map
map
## step 8: plot reviewers, germany
url <-
"http://biogeo.ucdavis.edu/data/gadm2/R/DEU_adm1.RData"
fname <- basename(url)
if ( !file.exists(fname) ){
download.file(url, fname, mode="wb")
}
load(fname)
map2 <-
ggplot(data=gadm, aes(x=long, y=lat)) +
geom_polygon(data = gadm, aes(group=group)) +
geom_path(color="white", aes(group=group)) +
geom_point(data = rev13_dat,
aes(x = lon, y = lat),
colour = "#F54B1A70", size=5, na.rm=T) +
coord_map(xlim=c(5, 16), ylim=c(47,55.5)) +
theme_bw()
map2
map2
library(stringr) # string processing
library(rvest) # scraping suite
library(d3Network) # visualizing networks
install.packages("d3Network")
library(stringr) # string processing
library(rvest) # scraping suite
library(d3Network) # visualizing networks
wd <- ("./wikipediaPolsci")
dir.create(wd)
setwd("../")
wd <- ("./wikipediaPolsci")
dir.create(wd)
setwd(wd)
url <- "http://en.wikipedia.org/wiki/List_of_political_scientists"
browseURL(url)
html <- read_html(url)
anchors <- html_nodes(html, xpath="//a")
length(anchors) # probably too many?
anchors <- html_nodes(html, xpath="//ul/li/a[1]")
links <- html_attr(anchors, "href")
anchors <- html_nodes(html, xpath="//ul/li/a[1]")
links <- html_attr(anchors, "href")
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Abramowitz")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "John_Zaller")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
links
##  step 3: extract names
names <- html_attr(anchors, "title")[links_index]
names <- str_replace(names, " \\(.*\\)", "")
names
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- html(fname)
}
warnings()
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
## step 5: identify links between political scientists
# loop preparation
connections <- data.frame(from=NULL, to=NULL)
# loop
for (i in seq_along(HTML)) {
pslinks <- html_attr(
html_nodes(HTML[[i]], xpath="//a"),
"href")
links_in_pslinks <- seq_along(links)[links %in% pslinks]
links_in_pslinks <- links_in_pslinks[links_in_pslinks!=i]
connections <- rbind(
connections,
data.frame(
from=rep(i, length(links_in_pslinks)),
to=links_in_pslinks
)
)
}
names(connections) <- c("from", "to")
head(connections)
# make symmetrical
connections <- rbind(
connections,
data.frame(from=connections$to,
to=connections$from)
)
## step 6: visualize connections
dir.create("figures")
d3SimpleNetwork(connections,
width = 1000,
height = 900,
file="figures/connections.html")
browseURL("figures/connections.html")
d3ForceNetwork(Links = connections, Nodes = data.frame(name = names), Source = "from",
Target = "to", opacity = 0.9, zoom = T, width = 1000, height = 900, file = "figures/connections_names.html")
browseURL('figures/connections_names.html')
?d3ForceNetwork
library(ROAuth)
library(RCurl)
library(twitteR)
library(streamR)
install.packages(c("ROAuth", "twitteR", "streamR"))
library(ROAuth)
library(RCurl)
library(twitteR)
library(streamR)
## directory --------------------
wd <- ("./twitterApis")
dir.create(wd)
setwd(wd)
api_key <- Sys.getenv("twitter_api_key")
credentials <- c(
"twitter_api_key=ERli0GK6ZtGr3n1diSGQ",
"twitter_api_secret=ZMHQ1lO6KTkokwxLheYw5V2GEBIpg5lsN4sc8pI4zLg",
"twitter_access_token=2178798121-LuPiAoNd8NS6PfzE0wFPPoUgt0bZyzUUb8ziaJP",
"twitter_access_token_secret=YeHgXKITIZrcZY91va5pYGvCBxIvoyAncXDvQHaHhGCIk"
)
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(credentials, fname)
