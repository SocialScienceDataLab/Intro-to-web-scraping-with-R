conditions <- unlist(xpathSApply(parsed_feed, xpath, xmlAttrs))
location <- t(xpathSApply(parsed_feed, "//yweather:location", xmlAttrs))
forecasts <- t(xpathSApply(parsed_feed, "//yweather:forecast", xmlAttrs))
forecast <- merge(location, forecasts)
options(yahooid = "v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL")
baseurl <- "http://where.yahooapis.com/v1/places.q('%s')"
woeid_url <- sprintf(baseurl, URLencode("Hoboken, NJ, USA")) # careful: URL encoding!
parsed_woeid <- xmlParse((getForm(woeid_url, appid = getOption("yahooid"))))
woeid <- xpathSApply(parsed_woeid, "//*[local-name()='locality1']", xmlAttrs)[2,] # careful: namespaces!
# build wrapper function
getWeather <- function(place = "New York", ask = "current", temp = "c") {
if (!ask %in% c("current","forecast")) {
stop("Wrong ask parameter. Choose either 'current' or 'forecast'.")
}
if (!temp %in%  c("c", "f")) {
stop("Wrong temp parameter. Choose either 'c' for Celsius or 'f' for Fahrenheit.")
}
## get woeid
base_url <- "http://where.yahooapis.com/v1/places.q('%s')"
woeid_url <- sprintf(base_url, URLencode(place))
parsed_woeid <- xmlParse((getForm(woeid_url, appid = getOption("yahooid"))))
woeid <- xpathSApply(parsed_woeid, "//*[local-name()='locality1']", xmlAttrs)[2,]
## get weather feed
feed_url <- "http://weather.yahooapis.com/forecastrss"
parsed_feed <- xmlParse(getForm(feed_url, .params = list(w = woeid, u = temp)))
## get current conditions
if (ask == "current") {
xpath <- "//yweather:location|//yweather:condition"
conds <- data.frame(t(unlist(xpathSApply(parsed_feed, xpath, xmlAttrs))))
message(sprintf("The weather in %s, %s, %s is %s. Current temperature is %sÂ°%s.", conds$city, conds$region, conds$country, tolower(conds$text), conds$temp, toupper(temp)))
}
## get forecast
if (ask == "forecast") {
location <- data.frame(t(xpathSApply(parsed_feed, "//yweather:location", xmlAttrs)))
forecasts <- data.frame(t(xpathSApply(parsed_feed, "//yweather:forecast", xmlAttrs)))
message(sprintf("Weather forecast for %s, %s, %s:", location$city, location$region, location$country))
return(forecasts)
}
}
getWeather(place = "Paris", ask = "current", temp = "c")
getWeather(place = "Bamberg", ask = "forecast", temp = "c")
yahooid = "v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL"
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(yahooid, fname)
Sys.getenv(yahooid)
### -----------------------------
### workshop exercises
### simon munzert
### -----------------------------
library(stringr)
library(rvest)
library(XML)
library(jsonlite)
library(RCurl)
Sys.getenv(yahooid)
yahooid = "v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL"
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(yahooid, fname)
normalizePath("~/")
yahooid = "yahooid=v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL"
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(yahooid, fname)
Sys.getenv(yahooid)
### -----------------------------
### workshop exercises
### simon munzert
### -----------------------------
library(stringr)
library(rvest)
library(XML)
library(jsonlite)
library(RCurl)
Sys.getenv(yahooid)
yahoo_id = "yahooid=v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL"
fname <- paste0(normalizePath("~/"),".Renviron")
fname
writeLines(yahoo_id, fname)
yahoo_id = "yahooid='v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL'"
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(yahoo_id, fname)
Sys.getenv(yahooid)
### -----------------------------
### workshop exercises
### simon munzert
### -----------------------------
library(stringr)
library(rvest)
library(XML)
library(jsonlite)
library(RCurl)
Sys.getenv(yahoo_id)
Sys.getenv(yahooid)
yahooid = c("yahoo_id=v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL")
yahoo_id = c("yahoo_id=v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL")
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(yahoo_id, fname)
getwd()
Sys.getenv("")
Sys.getenv("yahoo_id")
url <- "http://spiegel.de"
content <- html(url, encoding = "utf8")
library(rvest)
library(stringr)
library(ggmap)
# html() parses an HTML page; automatically starts a GET request
url <- "http://spiegel.de"
content <- html(url, encoding = "utf8")
class(content)
content
browseURL("http://selectorgadget.com/")
headlines <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "headline", " " ))]')
class(headlines)
headlines
html_text(headlines)
library(stringr)
str_replace(headlines, '\\\"', "'")
html_text(headlines)
headlines_string <- html_text(headlines)
headlines_string
str_replace(headlines_string, '\\\"', "'")
str_replace_all(headlines_string, '\\\"', "'")
html_table(content)
## packages ---------------------
library(ROAuth)
library(RCurl)
library(twitteR)
library(streamR)
## directory --------------------
wd <- ("./data/twitterApis")
dir.create(wd)
setwd(wd)
credentials <- c(
"twitter_api_key=auXJlLHKKNwkVSpyK7tK9TiMc",
"twitter_api_secret=327yJX4GOwuGzDFxeHQP936XGcQqJuUch2aGnSbK1hxKjBO2Y4",
"twitter_access_token=2734676077-CelbcJHC2syxUUyh22mAKbfvvxgdt1qeAZy0sTS",
"twitter_access_token_secret=FmM3yt7WYnae71mYKqTyUJFK43DbsbZJpR0L36hylcOkf"
)
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(credentials, fname)
# negotiate credentials
api_key <- Sys.getenv("twitter_api_key")
api_secret <- Sys.getenv("twitter_api_secret")
access_token <- Sys.getenv("twitter_access_token")
access_token_secret <- Sys.getenv("twitter_access_token_secret")
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")
tweets <- searchTwitter(searchString = "Pegida", n=25, lang=NULL, since=NULL, until=NULL, locale=NULL, geocode=NULL, sinceID=NULL, retryOnRateLimit=120)
tweets
tweets_df <- twListToDF(tweets)
head(tweets_df)
names(tweets_df)
# get information about users
user <- getUser("RDataCollection", cainfo=cainfo)
user$name
user$lastStatus
user$followersCount
user$statusesCount
user_followers <- user$getFollowers(cainfo=cainfo)
user_friends <- user$getFriends(cainfo=cainfo)
user_timeline <- userTimeline(user, n=20, cainfo=cainfo)
timeline_df <- twListToDF(user_timeline)
user <- getUser("RDataCollection", cainfo=cainfo)
user <- getUser("RDataCollection")
user$name
user$lastStatus
user$followersCount
user$statusesCount
user_followers <- user$getFollowers(cainfo=cainfo)
user_followers <- user$getFollowers()
user_friends <- user$getFriends()
user_timeline <- userTimeline(user, n=20)
timeline_df <- twListToDF(user_timeline)
getCurRateLimitInfo(cainfo=cainfo)
View(timeline_df)
getCurRateLimitInfo()
getwd()
dir()
load("twitter_auth.RData")
filterStream("tweets_pegida.json", track = c("Pegida"), timeout = 5, oauth = twitCred)
tweets <- parseTweets("tweets_pegida.json", simplify = TRUE)
names(tweets)
cat(tweets$text[1])
### -----------------------------
### exploring Wikipedia page view statistics
### simon munzert
### -----------------------------
## goals ------------------------
# fetch Wikipedia page view statistics
# explore them
## tasks ------------------------
# access statistics (JSON format)
# import JSON into R
# tidy data
# exploration
## packages ---------------------
library(rvest)
library(jsonlite)
library(stringr)
library(wikipediatrend)
getwd()
setwd("../../")
wd <- ("./data/wikipediaTrend")
dir.create(wd)
setwd(wd)
url  <- "http://stats.grok.se/json/en/latest90/Influenza"
json <- html_text(html(url))
# inspect data
cat(str_c(unlist(str_split(json, ",")),",\n")[1:13],"\n...")
data  <- fromJSON(json)
summary(data)
# tidy data
date  <- as.Date(names(data$daily_views))
views <- unlist(data$daily_views)
plot( date, views,
ylim = c(2000, 6000),
type = "h",
col  = "#F54B1A90",
lwd=3,
main="Influenza Page Views on Wikipedia (en)")
lines(lowess(views ~ date), col = "#1B346C90", lwd=5)
getwd()
setwd("../../")
wd <- ("./data/ieaSelenium")
dir.create(wd)
setwd(wd)
library(rvest)
browseURL("http://www.iea.org/policiesandmeasures/renewableenergy/")
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
content <- html(url)
library(RSelenium)
checkForServer()
startServer()
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
remDr$open()
remDr$navigate(url)
# open regions menu
xpath <- '//*[@id="advancedSearch"]/div[1]/div[1]/ul/li[1]/span'
regionsElem <- remDr$findElement(using = 'xpath', value = xpath)
openRegions <- regionsElem$clickElement() # click on button
# selection "European Union"
xpath <- '//*[@id="advancedSearch"]/div[1]/div[1]/ul/li[1]/ul/li[2]/label/input'
euElem <- remDr$findElement(using = 'xpath', value = xpath)
selectEU <- euElem$clickElement() # click on button
# set time frame
xpath <- '//*[@id="advancedSearch"]/div[2]/div[1]/select[1]'
fromDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
xpath <- '//*[@id="advancedSearch"]/div[2]/div[1]/select[2]'
toDrop <- remDr$findElement(using = 'xpath', value = xpath)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
# click on search button
xpath <- '//button[(((count(preceding-sibling::*) + 1) = 2) and parent::*)]'
searchElem <- remDr$findElement(using = 'xpath', value = xpath)
resultsPage <- searchElem$clickElement() # click on button
xpath <- '//*[@id="advancedSearch"]/div[1]/div[1]/ul/li[1]/span'
regionsElem <- remDr$findElement(using = 'xpath', value = xpath)
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > div:nth-child(1) > div:nth-child(1) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > div:nth-child(1) > div:nth-child(1) > ul:nth-child(2) > li:nth-child(1) > ul:nth-child(3) > li:nth-child(5) > label:nth-child(1) > input:nth-child(1)'
euElem <- remDr$findElement(using = 'css', value = css)
selectEU <- euElem$clickElement() # click on button
# set time frame
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > div:nth-child(2) > div:nth-child(1) > select:nth-child(2)'
fromDrop <- remDr$findElement(using = 'css', value = css)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > div:nth-child(2) > div:nth-child(1) > select:nth-child(3)'
toDrop <- remDr$findElement(using = 'css', value = css)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
# click on search button
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > button:nth-child(6)'
searchElem <- remDr$findElement(using = 'css', value = css)
resultsPage <- searchElem$clickElement() # click on button
output <- remDr$getPageSource(header = TRUE)
getwd()
?writeLines
writeLines(output[[1]], file = "iea-renewables.html")
write(output[[1]], file = "iea-renewables.html")
remDr$closeServer()
content <- html("iea-renewables.html", encoding = "utf8")
tabs <- html_table(content)
tabs <- html_table(content, fill = TRUE)
head(tabs[[4]])
tab <- head(tabs[[4]])
View(tab)
names(tab) <- c("title", "country", "year", "status", "type", "target")
source("rselenium-2048.r") # by Mark T. Patterson
grand.play()
getwd()
setwd("../../")
## packages ---------------------
library(rvest)
library(magrittr)
library(stringr)
library(tm)
## directory --------------------
wd <- ("./data/jstatsoftStats")
dir.create(wd)
setwd(wd)
## code -------------------------
## step 1: construct list of urls
baseurl <- "http://www.jstatsoft.org/"
volurl <- paste0("v", seq(1,62,1))
volurl[1:9] <- paste0("v0", seq(1, 9, 1))
brurl <- paste0("b0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "/", brurl)
names <- paste0(rep(volurl, each = 9), "_", brurl, ".html")
## step 2: download pages
folder <- "htmls_reviews/"
dir.create(folder)
for (i in 1:length(urls_list)) {
if (!file.exists(paste0(folder, names[i]))) {
download.file(urls_list[i], destfile = paste0(folder, "/", names[i]))
Sys.sleep(runif(1, 0, 1))
}
}
## step 3: access pages
listFiles <- list.files(folder, pattern = "v.*")
url <- character()
volume <- character()
reviewer <- character()
title <- character()
reference <- character()
authors <- character()
publisher <- character()
year <- numeric()
isbn <- character()
numDownload <- numeric()
date <- character()
for (i in 1:length(listFiles)) {
htmlOut <- html(paste0(folder, listFiles[i]), encoding = "utf8")
TableOut <- html_table(htmlOut)
if (length(TableOut) != 0) {
TableOut <- TableOut[[1]]
url[i] <- urls_list[i]
volume[i] <- names[i]
reviewer[i] <- TableOut[1,2]
#reference[i] <- TableOut[3,2]
authors[i] <- TableOut[5,2]
publisher[i] <- TableOut[7,2]
title[i] <- TableOut[6,2]
isbn[i] <- TableOut[8,2]
year[i] <- TableOut[9,2] %>% as.numeric
numDownload[i] <- TableOut[2,2] %>% str_extract(pattern = perl("(?<=\\()[[:digit:]]+")) %>% as.numeric
date[i] <- TableOut[3,2] %>% str_extract(pattern = "[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}$")
} else {
url[i] <- ""
volume[i] <- ""
reviewer[i] <- ""
#reference[i] <- ""
authors[i] <- ""
publisher[i] <- ""
title[i] <- ""
isbn[i] <- ""
year[i] <- NA
numDownload[i] <- NA
date[i] <- ""
}
}
## step 4: construct data frame
dat <- data.frame(url = url, volume = volume, reviewer = reviewer, authors = authors, publisher = publisher, title = title, isbn = isbn, year = year, numDownload = numDownload)
dat <- dat[dat$title!="",]
names(dat)
## step 5: data inspection
# publisher
dat$publisher <- as.character(dat$publisher)
sort(table(dat$publisher))
dat$publisher[str_detect(dat$publisher, "Wiley")] <- "Wiley"
dat$publisher[str_detect(dat$publisher, "Springer")] <- "Springer"
dat$publisher[str_detect(dat$publisher, "Chapman|CRC")] <- "Chapman Hall/CRC"
dat$publisher[str_detect(dat$publisher, "Reilly")] <- "O'Reilly"
dat$publisher[str_detect(dat$publisher, "Cambridge")] <- "Cambridge Univ. Press"
dat$publisher[str_detect(dat$publisher, "Manning")] <- "Manning"
dat$publisher[str_detect(dat$publisher, "World Scientific Publishing")] <- "World Scientific Publishing"
sort(table(dat$publisher))
# download statistics
dattop <- dat[order(dat$numDownload, decreasing = TRUE),]
dattop[1:10,]
summary(dat$numDownload)
hist(dat$numDownload, breaks=30)
plot(density(dat$numDownload), yaxt="n", ylab="", xlab="Number of downloads", main="Distribution of download statistics, book reviews")
# year
table(dat$year)
setwd("../../")
library(rvest)
library(stringr)
library(ggmap)
url <- "http://www.biermap24.de/brauereiliste.php"
browseURL(url)
library(stringr) # string processing
library(rvest) # scraping suite
library(d3Network) # visualizing networks
getwd()
wd <- ("./data/wikipediaPolsci")
dir.create(wd)
setwd(wd)
setwd("../../")
# Paket laden
library(stringr)
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
unlist(str_extract_all(example.obj, "tiny|sentence"))
unlist(str_extract(example.obj, "tiny|sentence"))
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
raw.data
name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}"))
name
name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]"))
name
unlist(str_extract_all(example.obj, "[[:punct:]]"))
unlist(str_extract_all(example.obj, "[:punct:]"))
install.packages("stringr")
install.packages("stringr")
library(stringr)
string_extract("iPhone 600$, shoes 200$", "[0-9]+\\$")
library(stringr)
string_extract("iPhone 600$, shoes 200$", "[0-9]+\\$")
str_extract("iPhone 600$, shoes 200$", "[0-9]+\\$")
str_extract_all("iPhone 600$, shoes 200$", "[0-9]+\\$")
unlist(str_extract_all("iPhone 600$, shoes 200$", "[0-9]+\\$"))
str_extract("hallo. mein name ist Daniel. Ich bin alt und wohne in Konstanz", "\\b[a-z]{1,4}\\b")
str_extract_all("hallo. mein name ist Daniel. Ich bin alt und wohne in Konstanz", "\\b[a-z]{1,4}\\b")
str_extract(c("log.txt", "script.R", "test.txt"), ".*?\\.txt$")
str_extract(c("test.txt log.txt", "script.R", "test.txt"), ".*?\\.txt$")
str_extract(c("07/05/2015", "01-02-2013"), "\\d{2}/\\d{2}/\\d{4}")
str_extract("<br>Hallo</br>", "<(.+?)>.+?</\\1>")
?agrep
library(stringr) # string processing
getwd()
library(rvest) # scraping suite
library(ggmap) # geocoding
wd <- ("./data/ajpsReviewers")
dir.create(wd)
setwd(wd)
url <- "http://ajps.org/list-of-reviewers/"
browseURL(url)
## step 2: retrieve pdfs
# get page
content <- html(url)
# get anchor (<a href=...>) nodes via xpath
anchors <- html_nodes(content, xpath = "//a")
# get value of anchors' href attribute
hrefs <- html_attr(anchors, "href")
# filter links to pdfs
pdfs <- hrefs[ str_detect(hrefs, "reviewers.*\\d{4}.*pdf") ]
pdfs <- pdfs[!is.na(pdfs)]
pdfs
# define names for pdfs on disk
pdf_names <- str_extract(pdfs, "\\d{4}.pdf")
pdf_names
fname <- str_replace(pdf_names[1], ".pdf", ".txt")
fname
rawdat <- readLines(fname)
getwd()
fname <- str_replace(pdf_names[2], ".pdf", ".txt")
rawdat <- readLines(fname)
head(rawdat)
## step 5: tidy data
rev13 <- rawdat %>%
str_c(collapse="") %>%
str_replace_all(pattern = "[!\f]", replacement = " ")  %>%
str_replace_all(pattern = "\\]", replacement = " ")  %>%
str_split(pattern = "\\)") %>%
unlist()
head(rev13)
rev13 <- rev13[-1]
rev13
names <- rev13 %>%
str_extract(pattern = "^.*?,") %>%
str_replace_all(pattern = " |,", replacement = " ") %>%
str_trim()
head(names)
institution <- rev13 %>%
str_extract(pattern = ",.*\\(") %>%
str_replace_all(pattern = " |\\(|^, ", replacement = " ") %>%
str_trim
head(institution)
reviews <- rev13 %>%
str_extract("\\(.*") %>%
str_extract("\\d+") %>%
as.numeric()
head(reviews)
rev13_dat <- data.frame(names = names, institution = institution, reviews = reviews)
head(rev13_dat)
## step 6: geocode reviewers/institutions
# geocoding takes a while -> save results
# 2500 requests allowed per day
if ( !file.exists("institutions2013_geo.RData")){
pos <- geocode(rev13_dat$institution)
geocodeQueryCheck()
save(pos, file="institutions2013_geo.RData")
} else {
load("institutions2013_geo.RData")
}
head(pos)
rev13_dat$lon <- pos$lon
rev13_dat$lat <- pos$lat
## step 7: plot reviewers, worldwide
mapWorld <- borders("world")
map <-
ggplot() +
mapWorld +
geom_point(aes(x=rev13_dat$lon, y=rev13_dat$lat) ,
color="#F54B1A90", size=3 ,
na.rm=T) +
theme_bw() +
coord_map(xlim=c(-180, 180), ylim=c(-60,70))
map
