numDownload[i] <- TableOut[2,2] %>% str_extract(pattern = perl("(?<=\\()[[:digit:]]+")) %>% as.numeric
date[i] <- TableOut[3,2] %>% str_extract(pattern = "[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}$")
} else {
url[i] <- ""
volume[i] <- ""
reviewer[i] <- ""
#reference[i] <- ""
authors[i] <- ""
publisher[i] <- ""
title[i] <- ""
isbn[i] <- ""
year[i] <- NA
numDownload[i] <- NA
date[i] <- ""
}
}
## step 4: construct data frame
dat <- data.frame(url = url, volume = volume, reviewer = reviewer, authors = authors, publisher = publisher, title = title, isbn = isbn, year = year, numDownload = numDownload)
dat <- dat[dat$title!="",]
names(dat)
## step 5: data inspection
# publisher
dat$publisher <- as.character(dat$publisher)
sort(table(dat$publisher))
dat$publisher[str_detect(dat$publisher, "Wiley")] <- "Wiley"
dat$publisher[str_detect(dat$publisher, "Springer")] <- "Springer"
dat$publisher[str_detect(dat$publisher, "Chapman|CRC")] <- "Chapman Hall/CRC"
dat$publisher[str_detect(dat$publisher, "Reilly")] <- "O'Reilly"
dat$publisher[str_detect(dat$publisher, "Cambridge")] <- "Cambridge Univ. Press"
dat$publisher[str_detect(dat$publisher, "Manning")] <- "Manning"
dat$publisher[str_detect(dat$publisher, "World Scientific Publishing")] <- "World Scientific Publishing"
sort(table(dat$publisher))
# download statistics
dattop <- dat[order(dat$numDownload, decreasing = TRUE),]
dattop[1:10,]
summary(dat$numDownload)
hist(dat$numDownload, breaks=30)
plot(density(dat$numDownload), yaxt="n", ylab="", xlab="Number of downloads", main="Distribution of download statistics, book reviews")
# year
table(dat$year)
setwd("../../")
library(rvest)
library(stringr)
library(ggmap)
url <- "http://www.biermap24.de/brauereiliste.php"
browseURL(url)
## tasks ------------------------
# downloading PDF files
# importing them into R (as plain text)
# extract information via regex
# geocoding
## packages ---------------------
library(stringr) # string processing
library(rvest) # scraping suite
library(ggmap) # geocoding
## directory ---------------------
wd <- ("./data/ajpsReviewers")
dir.create(wd)
setwd(wd)
url <- "http://ajps.org/list-of-reviewers/"
browseURL(url)
## code ---------------------
## step 1: inspect page
url <- "http://ajps.org/list-of-reviewers/"
browseURL(url)
## step 2: retrieve pdfs
# get page
content <- html(url)
# get anchor (<a href=...>) nodes via xpath
anchors <- html_nodes(content, xpath = "//a")
# get value of anchors' href attribute
hrefs <- html_attr(anchors, "href")
# filter links to pdfs
pdfs <- hrefs[ str_detect(hrefs, "reviewers.*\\d{4}.*pdf") ]
pdfs <- pdfs[!is.na(pdfs)]
pdfs
# define names for pdfs on disk
pdf_names <- str_extract(pdfs, "\\d{4}.pdf")
pdf_names
# download pdfs
for(i in seq_along(pdfs)) {
download.file(pdfs[i], pdf_names[i], mode="wb")
}
pdfs
## step 3: transform pdfs into txt data
# xpdf: http://www.foolabs.com/xpdf/download.html
# function working for windows ...
# should use system() instead of shell() on Mac/Linux
pdftotext <- function(fname){
path_to_pdftotext <-
"C:/xpdf/pdftotext.exe"
fname_txt <- str_replace(fname, ".pdf", ".txt")
command <- str_c(path_to_pdftotext,
fname,
fname_txt, sep=" ")
shell(command)
}
pdftotext(pdf_names[1])
pdftotext(pdf_names[2])
pdftotext(pdf_names[3])
pdftotext(pdf_names[4])
pdftotext(pdf_names[5])
### -----------------------------
### ajps reviewers
### simon munzert
### -----------------------------
## goals ------------------------
# fetch list of AJPS reviewers from PDFs
# locate them on a map
## tasks ------------------------
# downloading PDF files
# importing them into R (as plain text)
# extract information via regex
# geocoding
## packages ---------------------
library(stringr) # string processing
library(rvest) # scraping suite
library(ggmap) # geocoding
## directory ---------------------
wd <- ("./data/ajpsReviewers")
dir.create(wd)
setwd(wd)
## code ---------------------
## step 1: inspect page
url <- "http://ajps.org/list-of-reviewers/"
browseURL(url)
## step 2: retrieve pdfs
# get page
content <- html(url)
# get anchor (<a href=...>) nodes via xpath
anchors <- html_nodes(content, xpath = "//a")
# get value of anchors' href attribute
hrefs <- html_attr(anchors, "href")
# filter links to pdfs
pdfs <- hrefs[ str_detect(hrefs, "reviewers.*\\d{4}.*pdf") ]
pdfs <- pdfs[!is.na(pdfs)]
pdfs
# define names for pdfs on disk
pdf_names <- str_extract(pdfs, "\\d{4}.pdf")
pdf_names
# download pdfs
for(i in seq_along(pdfs)) {
download.file(pdfs[i], pdf_names[i], mode="wb")
}
## step 3: transform pdfs into txt data
# xpdf: http://www.foolabs.com/xpdf/download.html
# function working for windows ...
# should use system() instead of shell() on Mac/Linux
pdftotext <- function(fname){
path_to_pdftotext <-
"C:/xpdf/pdftotext.exe"
fname_txt <- str_replace(fname, ".pdf", ".txt")
command <- str_c(path_to_pdftotext,
fname,
fname_txt, sep=" ")
shell(command)
}
pdftotext(pdf_names[1])
pdftotext(pdf_names[2])
pdftotext(pdf_names[3])
pdftotext(pdf_names[4])
pdftotext(pdf_names[5])
list.files()
?list.files
list.files(pattern = "txt")
txt_names <- list.files(pattern = "txt")
txt_names <- list.files(pattern = "txt")
txt_names
rawdat <- readLines(txt_names[4])
head(rawdat)
rev13 <- rawdat %>%
str_c(collapse="") %>% str_replace_all(pattern = "[!\f]", replacement = " ")
rev13
rev13 <- rawdat %>%
str_c(collapse="")
rev13
rev13 <- rawdat %>%
str_c(collapse="") %>%
str_replace_all(pattern = "[!\f]", replacement = " ")  %>%
str_replace_all(pattern = "\\]", replacement = " ")
rev13
rev13 <- rawdat %>%
str_c(collapse="") %>%
str_replace_all(pattern = "[!\f]", replacement = " ")  %>%
str_replace_all(pattern = "\\]", replacement = " ") %>%
str_split(pattern = "\\)") %>%
unlist()
rev13
head(rev13)
names <- rev13 %>%
str_extract(pattern = "^.*?,") %>%
str_replace_all(pattern = " |,", replacement = " ") %>%
str_trim()
head(names)
rev13 <- rev13[-1]
institution <- rev13 %>%
str_extract(pattern = ",.*\\(") %>%
str_replace_all(pattern = " |\\(|^, ", replacement = " ") %>%
str_trim
head(institution)
reviews <- rev13 %>%
str_extract("\\(.*") %>%
str_extract("\\d+") %>%
as.numeric()
head(reviews)
rev13_dat <- data.frame(names = names, institution = institution, reviews = reviews)
head(rev13_dat)
## step 5: tidy data
rev13 <- rawdat %>%
str_c(collapse="") %>%
str_replace_all(pattern = "[!\f]", replacement = " ")  %>%
str_replace_all(pattern = "\\]", replacement = " ") %>%
str_split(pattern = "\\)") %>%
unlist()
head(rev13)
rev13 <- rev13[-1]
names <- rev13 %>%
str_extract(pattern = "^.*?,") %>%
str_replace_all(pattern = " |,", replacement = " ") %>%
str_trim()
head(names)
institution <- rev13 %>%
str_extract(pattern = ",.*\\(") %>%
str_replace_all(pattern = " |\\(|^, ", replacement = " ") %>%
str_trim
head(institution)
reviews <- rev13 %>%
str_extract("\\(.*") %>%
str_extract("\\d+") %>%
as.numeric()
head(reviews)
rev13_dat <- data.frame(names = names, institution = institution, reviews = reviews)
head(rev13_dat)
## step 6: geocode reviewers/institutions
# geocoding takes a while -> save results
# 2500 requests allowed per day
if ( !file.exists("institutions2013_geo.RData")){
pos <- geocode(rev13_dat$institution)
geocodeQueryCheck()
save(pos, file="institutions2013_geo.RData")
} else {
load("institutions2013_geo.RData")
}
head(pos)
rev13_dat$lon <- pos$lon
rev13_dat$lat <- pos$lat
## step 7: plot reviewers, worldwide
mapWorld <- borders("world")
map <-
ggplot() +
mapWorld +
geom_point(aes(x=rev13_dat$lon, y=rev13_dat$lat) ,
color="#F54B1A90", size=3 ,
na.rm=T) +
theme_bw() +
coord_map(xlim=c(-180, 180), ylim=c(-60,70))
map
## step 8: plot reviewers, germany
url <-
"http://biogeo.ucdavis.edu/data/gadm2/R/DEU_adm1.RData"
fname <- basename(url)
if ( !file.exists(fname) ){
download.file(url, fname, mode="wb")
}
load(fname)
map2 <-
ggplot(data=gadm, aes(x=long, y=lat)) +
geom_polygon(data = gadm, aes(group=group)) +
geom_path(color="white", aes(group=group)) +
geom_point(data = rev13_dat,
aes(x = lon, y = lat),
colour = "#F54B1A70", size=5, na.rm=T) +
coord_map(xlim=c(5, 16), ylim=c(47,55.5)) +
theme_bw()
map2
getwd()
setwd("../../")
## packages ---------------------
library(stringr) # string processing
library(rvest) # scraping suite
library(d3Network) # visualizing networks
## directory --------------------
wd <- ("./data/wikipediaPolsci")
dir.create(wd)
setwd(wd)
url <- "http://en.wikipedia.org/wiki/List_of_political_scientists"
browseURL(url)
html <- html(url)
anchors <- html_nodes(html, xpath="//a")
length(anchors) # probably too many?
url <- "http://en.wikipedia.org/wiki/List_of_political_scientists"
browseURL(url)
html <- html(url)
anchors <- html_nodes(html, xpath="//a")
length(anchors) # probably too many?
anchors <- html_nodes(html, xpath="//ul/li/a[1]")
links <- html_attr(anchors, "href")
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Abramowitz")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "John_Zaller")] &
str_detect(links, "/wiki/")
links_iffer
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
links
anchors
names <- html_attr(anchors, "title")[links_index]
names <- str_replace(names, " \\(.*\\)", "")
names <- iconv(names, "utf8", "latin1")
names
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
getwd()
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- html(fname)
}
## step 5: identify links between political scientists
# loop preparation
connections <- data.frame(from=NULL, to=NULL)
# loop
for (i in seq_along(HTML)) {
pslinks <- html_attr(
html_nodes(HTML[[i]], xpath="//a"),
"href")
links_in_pslinks <- seq_along(links)[links %in% pslinks]
links_in_pslinks <- links_in_pslinks[links_in_pslinks!=i]
connections <- rbind(
connections,
data.frame(
from=rep(i, length(links_in_pslinks)),
to=links_in_pslinks
)
)
}
# results
names(connections) <- c("from", "to")
head(connections)
# make symmetrical
connections <- rbind(
connections,
data.frame(from=connections$to,
to=connections$from)
)
connections
## step 6: visualize connections
d3SimpleNetwork(connections,
width = 1000,
height = 900,
file="connections.html")
browseURL("connections.html")
## step 6: visualize connections
d3SimpleNetwork(connections,
width = 1000,
height = 900,
file="output/connections.html")
dir.create("figures")
d3SimpleNetwork(connections,
width = 1000,
height = 900,
file="figures/connections.html")
browseURL("figures/connections.html")
d3ForceNetwork(Links = connections, Nodes = data.frame(name = names), Source = "from",
Target = "to", opacity = 0.9, zoom = T, width = 1000, height = 900, file = "connections_names.html")
browseURL('figures/connections_names.html')
## step 6: visualize connections
dir.create("figures")
d3SimpleNetwork(connections,
width = 1000,
height = 900,
file="figures/connections.html")
browseURL("figures/connections.html")
d3ForceNetwork(Links = connections, Nodes = data.frame(name = names), Source = "from",
Target = "to", opacity = 0.9, zoom = T, width = 1000, height = 900, file = "figures/connections_names.html")
browseURL('figures/connections_names.html')
setwd("../../")
### -----------------------------
### exploring Wikipedia page view statistics
### simon munzert
### -----------------------------
## goals ------------------------
# fetch Wikipedia page view statistics
# explore them
## tasks ------------------------
# access statistics (JSON format)
# import JSON into R
# tidy data
# exploration
## packages ---------------------
library(rvest)
library(jsonlite)
library(stringr)
library(wikipediatrend)
## directory --------------------
wd <- ("./data/wikipediaTrend")
dir.create(wd)
setwd(wd)
## code -------------------------
## step 1: manually access data
# fetch data
url  <- "http://stats.grok.se/json/en/latest90/Pegida"
json <- html_text(html(url))
cat(str_c(unlist(str_split(json, ",")),",\n")[1:13],"\n...")
data  <- fromJSON(json)
summary(data)
date  <- as.Date(names(data$daily_views))
views <- unlist(data$daily_views)
plot( date, views,
type = "h",
col  = "#F54B1A90",
lwd=3,
main="Pegida Page Views on Wikipedia (en)")
lines(lowess(views ~ date, f=.1), col = "#1B346C90", lwd=5)
pegida <- wp_trend(page = "Pegida", from = Sys.Date() - 30, to = Sys.Date(), lang = "en", friendly = T, requestFrom = "anonymous", userAgent = F)
class(pegida)
plot(pegida)
plot(pegida, pch = 19)
plot(pegida, pch = 19, type = "l")
pegida <- wp_trend(page = "Pegida", from = Sys.Date() - 150, to = Sys.Date(), lang = "en", friendly = T, requestFrom = "anonymous", userAgent = F)
class(pegida)
plot(pegida, pch = 19, type = "l")
pegida <- wp_trend(page = "Ebola", from = Sys.Date() - 150, to = Sys.Date(), lang = "en", friendly = T, requestFrom = "anonymous", userAgent = F)
class(pegida)
plot(pegida, pch = 19, type = "l")
getwd()
setwd("../../")
### -----------------------------
### exploring twitter's apis
### simon munzert
### -----------------------------
## goals ------------------------
# explore Twitter's APIs
## packages ---------------------
library(ROAuth)
library(RCurl)
library(twitteR)
library(streamR)
## directory --------------------
wd <- ("./data/twitterApis")
dir.create(wd)
setwd(wd)
## apis ----------
load("twitter_auth.RData")
filterStream("tweets_stream.json", track = c("berlin"), timeout = 10, oauth = twitCred)
filterStream("tweets_stream.json", track = c("halligalli"), timeout = 10, oauth = twitCred)
filterStream("tweets_stream.json", track = c("halligalli"), timeout = 10, oauth = twitCred)
## packages ---------------------
library(RSelenium)
library(rvest)
getwd()
setwd("../../")
wd <- ("./data/ieaSelenium")
dir.create(wd)
setwd(wd)
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
browseURL(url)
checkForServer()
# start server
startServer()
# connect to server
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
remDr$open()
remDr$navigate(url)
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > div:nth-child(1) > div:nth-child(1) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > div:nth-child(1) > div:nth-child(1) > ul:nth-child(2) > li:nth-child(1) > ul:nth-child(3) > li:nth-child(5) > label:nth-child(1) > input:nth-child(1)'
euElem <- remDr$findElement(using = 'css', value = css)
selectEU <- euElem$clickElement() # click on button
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > div:nth-child(2) > div:nth-child(1) > select:nth-child(2)'
fromDrop <- remDr$findElement(using = 'css', value = css)
clickFrom <- fromDrop$clickElement() # click on drop-down menu
writeFrom <- fromDrop$sendKeysToElement(list("2000")) # enter start year
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > div:nth-child(2) > div:nth-child(1) > select:nth-child(3)'
toDrop <- remDr$findElement(using = 'css', value = css)
clickTo <- toDrop$clickElement() # click on drop-down menu
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > div:nth-child(2) > div:nth-child(1) > select:nth-child(3)'
toDrop <- remDr$findElement(using = 'css', value = css)
clickTo <- toDrop$clickElement() # click on drop-down menu
writeTo <- toDrop$sendKeysToElement(list("2010")) # enter end year
css <- '.middleContainer > div:nth-child(1) > form:nth-child(6) > button:nth-child(6)'
searchElem <- remDr$findElement(using = 'css', value = css)
resultsPage <- searchElem$clickElement() # click on button
# store index page
output <- remDr$getPageSource(header = TRUE)
write(output[[1]], file = "iea-renewables.html")
getwd()
remDr$closeServer()
content <- html("iea-renewables.html", encoding = "utf8")
tabs <- html_table(content, fill = TRUE)
tab <- head(tabs[[4]])
names(tab) <- c("title", "country", "year", "status", "type", "target")
tab <- head(tabs[[4]])
names(tab) <- c("title", "country", "year", "status", "type", "target")
head(tab)
